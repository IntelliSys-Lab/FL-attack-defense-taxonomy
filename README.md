# FL-attack-defense-taxonomy

## Attack

|                   | Data Poisoning Attack                                        | Model Poisoning Attack                                       |
| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Untargeted Attack | [[1-1]](#1-1) [[1-2]](#1-2) [[1-3]](#1-3)                    | [[2-1]](#2-1) [[2-2]](#2-2) [[2-3]](#2-3) [[2-4]](#2-4) [[2-5]](#2-5) |
| Targeted Attack   | [[3-1]](#3-1) [[3-2]](#3-2) [[3-3]](#3-3) [[3-4]](#3-4) [[3-5]](#3-5) | [[4-1]](#4-1) [[4-2]](#4-2) [[4-3]](#4-3) [[4-4]](#4-4) [[4-5]](#4-5) [[4-6]](#4-6) [[4-7]](#4-7) [[4-8]](#4-8) |

<a id="1-1">[1-1] Data poisoning attacks against federated learning systems. arXiv preprint; </a>

<a id="1-2">[1-2] On the Performance Impact of Poisoning Attacks on Load Forecasting in Federated Learning. UbiComp 2021; </a>

<a id="1-3">[1-3] Poisoning attacks on federated learning-based iot intrusion detection system. NDSS workshop 2020; </a>



<a id="2-1">[2-1] Local model poisoning attacks to byzantine robust federated learning. USENIX Security 2020; </a>

<a id="2-2">[2-2]  Breaking byzantine-tolerant sgd by inner product manipulation. UAI 2020; </a>

<a id="2-3">[2-3] Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning. NDSS 2021; </a>

<a id="2-4">[2-4] Desmp: Differential privacy-exploited stealthy model poisoning attacks in federated learning. arXiv preprint;</a>

<a id="2-5">[2-5] A Little Is Enough: Circumventing Defenses For Distributed Learning. NIPS 2019;</a>



<a id="3-1">[3-1] Attack of the tails: Yes, you really can backdoor federated learning. NIPS 2020; </a>

<a id="3-2">[3-2] Federated learning in adversarial settings. arXiv; </a>

<a id="3-3">[3-3] Deep model poisoning attack on federated learning. Future Internet; </a>

<a id="3-4">[3-4] Poisoning Attacks in Federated Learning: An Evaluation on Traffic Sign Classification. CODASPY 2020; </a>

<a id="3-5">[3-5] Backdoor attacks on federated meta-learning. NIPS 2020; </a>



<a id="4-1">[4-1] Analyzing federated learning through an adversarial lens. ICML 2019; </a>

<a id="4-2">[4-2] How to backdoor federated learning. AISTATS 2020; </a>

<a id="4-3">[4-3] DBA: Distributed backdoor attacks against federated learning. ICLR 2019; </a>

<a id="4-4">[4-4] Dynamic backdoor attacks against federated learning. arXiv; </a>

<a id="4-5">[4-5] Generative poisoning attacks against federated learning in edge computing systems. IoTJ 2020; </a>

<a id="4-6">[4-6] Poisoning Attack in Federated Learning using Generative Adversarial Nets. TrustCom 2019; </a>

<a id="4-7">[4-7]  Can You Really Backdoor Federated Learning? arXiv;</a>

<a id="4-8">[4-8] Towards multi-party targeted model poisoning attacks against federated learning systems. High-Confidence Computing; </a>



## Defense

**Remove malicious updates**:

Federated Transfer Learning for Intelligent Fault Diagnostics Using Deep Adversarial Networks with Data Privacy. 

CRFL: Certifiably robust federated learning against backdoor attacks. ICML 2021;

Federatedreverse: A detection and defense method against backdoor attacks in federated learning.

Robust federated learning with attack adaptive aggregation. 

Safelearning: Enable backdoor detectability in federated learning with secure aggregation.

Attack-resistant federated learning with residual-based reweighting. 

Privacy-enhanced federated learning against poisoning adversaries.

Siren: Byzantine-robust federated learning via proactive alarming



**Reduce the poisoning effect:**

Fl-wbc: Enhancing robustness against model poisoning attacks in federated learning from a client perspective.

Defending against backdoors in federated learning with robust learning rate. 

Baffle: Backdoor detection via feedback-based federated learning.

Robust aggregation for federated learning. arXiv 2019;

FLTrust Byzantine-robust Federated Learning via Trust Bootstrapping

Contra: Defending against poisoning attacks in federated learning.

Chain-afl: Chained adversarial-aware federated learning framework.

Moat: Model agnostic defense against targeted poisoning attacks in federated learning.

Tesseract: Gradient flip score to secure federated learning against model poisoning attacks. 

Batfl: Backdoor detection on federated learning in e-health.

Lomar: A local defense against poisoning attack on federated learning.



Machine learning with adversaries: Byzantine tolerant gradient descent. NIPS 2017;

Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer. arXiv preprint;

The Hidden Vulnerability of Distributed Learning in Byzantium. ICML 2018; 

Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning. NDSS 2021;

Byzantine-robust distributed learning: Towards optimal statistical rates. ICML 2018;

Draco: Byzantine-resilient distributed training via redundant gradients. ICML 2018.

Generalized byzantine-tolerant sgd. arXiv preprint;

signSGD with Majority Vote is Communication Efficient and Fault Tolerant. ICLR 2018;

Can You Really Backdoor Federated Learning? arXiv;

RSA: Byzantine robust stochastic aggregation methods for distributed learning from heterogeneous datasets. AAAI 2019;

Provably Secure Federated Learning against Malicious Clients. AAAI 2021;

Salvaging federated learning by local adaptation. arXiv 2020;

Ditto: Fair and robust federated learning through personalization. ICML 2021;



